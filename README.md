# Natural Language Processing - Cristiano De Nobili

This is the second part of the Deep Learning Course for the [Master in High-Performance Computing](https://twitter.com/mhpc_sissa_ictp) (SISSA/ICTP). It is about Natural Language processing, in particular on recent progress involving transformers-based models. I must thank the innovative-startup [AINDO](https://twitter.com/aindo_ai) for the support.

[Cristiano](https://denocris.com/) holds a Ph.D. in Theoretical Physics ([SISSA](https://twitter.com/Sissaschool)) and he has been actively working in Deep Learning for four years. In particular, he is now part of the Bixby project, Samsung's vocal assistant. He is also a TEDx speaker (here is talk about [AI, Humans and their future](https://youtu.be/8-hrmer9d_E)) and civil pilot (PPL). Here his contacts:

* If you are interested in science and tech news: [LinkedIn](https://www.linkedin.com/in/cristiano-de-nobili/) & [Twitter](https://twitter.com/denocris);
* On my [website](https://denocris.com/) you can find all my lectures, workshops, and talks;
* My [Instagram](https://www.instagram.com/denocris/?hl=it) is about flying, traveling, and adventure. It is the social platform that I use the most.

Have also a look at the first part of the course, [Introduction to Neural Networks (with PyTorch)](https://github.com/sissa/p2.13_seed), by [Alessio Ansuini](https://www.linkedin.com/in/alessioansuini/), and the third part, [Deep generative models with TensorFlow 2](https://gitlab.developers.cam.ac.uk/pc620/dl_course), by Piero Coronica.



## Course Outline

You can find [here](https://drive.google.com/drive/folders/1rbtfRdvwn9kiMXFrB_4KiUEaoD1dAoMF?usp=sharing) the videos of the lectures. For this year, I decided to use [PyTorch](https://pytorch.org/) as the main Deep Learning library.

* **Lecture 1:** intro to NLP, text preprocessing, [spaCy](https://spacy.io/), common problems in NLP (NER, POS, sentence classification, ...), non-contextual word embedding, SkipGram Word2Vec coded from scratch, pre-trained Glove with Gensim, intro to contextual word embedding and (self-)Attention Mechanism.

* **Lecture 2:** transfer learning main concepts, transformer-based model, how BERT-like models are trained and fine-tuned on downstream tasks, intro to [Transformers](https://github.com/huggingface/transformers) library Hugging Face, tokenization, language modeling with English and non-English (Italian Gilberto and Umberto) pre-trained AutoModels, some examples of NLP problems using Transformers Pipeline.

* **Lecture 3:** fine-tune a pre-trained Italian RoBERTa to solve word-sense disambiguation, [embedding geometry](https://arxiv.org/abs/1906.02715), clustering (TSNE and UMAP) and visualization (this lecture is a bit advanced). Part of this notebook is done using [PyTorch Lightning](https://github.com/PyTorchLightning/pytorch-lightning).

Useful links and references are inside each notebook. For any doubts or questions feel free to contact me!

